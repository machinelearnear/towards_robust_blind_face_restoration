{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db6e8764-d868-4166-b0bf-5785525f019f",
   "metadata": {},
   "source": [
    "# Robust Blind Face Restoration\n",
    "- Source: https://github.com/sczhou/CodeFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cea79f-3472-478f-810e-c1de0ebabbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "except ImportError:\n",
    "    !pip install torch\n",
    "    !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce9078-dadf-45bf-8812-4c73584eb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Using GPU')\n",
    "    device = 'cuda'\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print('CUDA not available. Please connect to a GPU instance if possible.')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa7523-319e-4a77-ac5d-abb242424287",
   "metadata": {},
   "source": [
    "## Download your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4300d7-ee17-44c4-bec7-199aa416c2cf",
   "metadata": {},
   "source": [
    "### Original video from YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b9e4e-b584-4aee-a1b2-b85dab931852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists as path_exists\n",
    "\n",
    "if not path_exists('helper.py') and not 'CodeFormer' in os.getcwd():\n",
    "    !wget https://raw.githubusercontent.com/machinelearnear/towards_robust_blind_face_restoration/main/helper.py\n",
    "\n",
    "from helper import vid2frames, get_yt_video_id\n",
    "\n",
    "youtube_url = 'https://www.youtube.com/watch?v=7K-Hk1qt_mk&t=0s&ab_channel=Entretenimientoyalgomas'\n",
    "video_name = f'videos/{get_yt_video_id(youtube_url)}.mp4'\n",
    "\n",
    "if not path_exists(video_name):\n",
    "    skip_frames, path_frames = vid2frames(youtube_url, sample_all_frames=False, format_note='240p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b41ab2-db6b-41dc-b421-0ebebd696f52",
   "metadata": {},
   "source": [
    "### Preview downloaded video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e605a31-ed4f-4218-a119-200fae1be1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(open(video_name, 'rb').read()).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891be508-e6a4-428b-9407-e7934c6dba7b",
   "metadata": {},
   "source": [
    "## Blind Face Restoration using [CodeFormer](https://github.com/sczhou/CodeFormer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71497dea-6de1-4724-9bc2-d96a8e5b7e93",
   "metadata": {},
   "source": [
    "### Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73dfac4-e2de-49ce-a53d-8e97bbcdcdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/studio-lab-user/towards_robust_blind_face_restoration'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not 'CodeFormer' in os.getcwd():\n",
    "    %cd CodeFormer\n",
    "    if not path_exists('CodeFormer'):\n",
    "        !git clone https://github.com/sczhou/CodeFormer.git\n",
    "\n",
    "try:\n",
    "    import basicsr\n",
    "    import facelib\n",
    "except ImportError:\n",
    "    # Install python dependencies\n",
    "    !pip install -r requirements.txt\n",
    "\n",
    "    # Install basicsr\n",
    "    !python basicsr/setup.py develop\n",
    "\n",
    "    # Download the pre-trained model\n",
    "    !python scripts/download_pretrained_models.py facelib\n",
    "    !python scripts/download_pretrained_models.py CodeFormer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382fb5b-f261-443f-9103-11b92d5ef613",
   "metadata": {},
   "source": [
    "### Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98dd380d-d377-46e7-9a22-182a670bbd2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeFormer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetcwd():\n\u001b[1;32m      2\u001b[0m     get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCodeFormer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "if not 'CodeFormer' in os.getcwd():\n",
    "    %cd CodeFormer\n",
    "    \n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import glob\n",
    "import torch\n",
    "from torchvision.transforms.functional import normalize\n",
    "from basicsr.utils import imwrite, img2tensor, tensor2img\n",
    "from basicsr.utils.download_util import load_file_from_url\n",
    "from facelib.utils.face_restoration_helper import FaceRestoreHelper\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from basicsr.utils.registry import ARCH_REGISTRY\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pretrain_model_url = {\n",
    "    'restoration': 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515636b7-a7e8-452b-a514-3d032d7f7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    w = 0.7\n",
    "    upscale = 2\n",
    "    test_path = '../frames'\n",
    "    has_aligned = False\n",
    "    only_center_face = False\n",
    "    draw_box = False\n",
    "    bg_upsampler = 'realesrgan'\n",
    "    bg_tile = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354c998-28d4-459a-a9e4-9b087f7ec7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ input & output ------------------------\n",
    "if args.test_path.endswith('/'):  # solve when path ends with /\n",
    "    args.test_path = args.test_path[:-1]\n",
    "\n",
    "w = args.w\n",
    "result_root = f'results/{os.path.basename(args.test_path)}_{w}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1149deb-7c7f-4816-89ce-0f1b4360f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ set up background upsampler ------------------\n",
    "if args.bg_upsampler == 'realesrgan':\n",
    "    if not torch.cuda.is_available():  # CPU\n",
    "        import warnings\n",
    "        warnings.warn('The unoptimized RealESRGAN is slow on CPU. We do not use it. '\n",
    "                      'If you really want to use it, please modify the corresponding codes.',\n",
    "                      category=RuntimeWarning)\n",
    "        bg_upsampler = None\n",
    "    else:\n",
    "        from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "        from basicsr.utils.realesrgan_utils import RealESRGANer\n",
    "        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n",
    "        bg_upsampler = RealESRGANer(\n",
    "            scale=2,\n",
    "            model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n",
    "            model=model,\n",
    "            tile=args.bg_tile,\n",
    "            tile_pad=10,\n",
    "            pre_pad=0,\n",
    "            half=True)  # need to set False in CPU mode\n",
    "else:\n",
    "    bg_upsampler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f6ddb-6b31-4de3-8e6f-2e7ed2141507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ set up CodeFormer restorer -------------------\n",
    "net = ARCH_REGISTRY.get('CodeFormer')(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9, \n",
    "                                        connect_list=['32', '64', '128', '256']).to(device)\n",
    "\n",
    "# ckpt_path = 'weights/CodeFormer/codeformer.pth'\n",
    "ckpt_path = load_file_from_url(url=pretrain_model_url['restoration'], \n",
    "                                model_dir='weights/CodeFormer', progress=True, file_name=None)\n",
    "checkpoint = torch.load(ckpt_path)['params_ema']\n",
    "net.load_state_dict(checkpoint)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467fe708-e14d-491f-a5e9-56187189d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ set up FaceRestoreHelper -------------------\n",
    "# large det_model: 'YOLOv5l', 'retinaface_resnet50'\n",
    "# small det_model: 'YOLOv5n', 'retinaface_mobile0.25'\n",
    "face_helper = FaceRestoreHelper(\n",
    "    args.upscale,\n",
    "    face_size=512,\n",
    "    crop_ratio=(1, 1),\n",
    "    det_model = 'YOLOv5l',\n",
    "    save_ext='png',\n",
    "    use_parse=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec21d91-e948-41e8-9dd7-f4a01f4f3f0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -------------------- start to processing ---------------------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# scan all the jpg and png images\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mtest_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.[jp][pn]g\u001b[39m\u001b[38;5;124m'\u001b[39m)))):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# clean all the intermediate results to process the next image\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     face_helper\u001b[38;5;241m.\u001b[39mclean_all()\n\u001b[1;32m      7\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------- start to processing ---------------------\n",
    "# scan all the jpg and png images\n",
    "for img_path in tqdm(sorted(glob.glob(os.path.join(args.test_path, '*.[jp][pn]g')))):\n",
    "    # clean all the intermediate results to process the next image\n",
    "    face_helper.clean_all()\n",
    "\n",
    "    img_name = os.path.basename(img_path)\n",
    "    # print(f'Processing: {img_name}')\n",
    "    basename, ext = os.path.splitext(img_name)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "    if args.has_aligned: \n",
    "        # the input faces are already cropped and aligned\n",
    "        img = cv2.resize(img, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
    "        face_helper.cropped_faces = [img]\n",
    "    else:\n",
    "        face_helper.read_image(img)\n",
    "        # get face landmarks for each face\n",
    "        num_det_faces = face_helper.get_face_landmarks_5(\n",
    "            only_center_face=args.only_center_face, resize=640, eye_dist_threshold=5)\n",
    "        # print(f'\\tdetect {num_det_faces} faces')\n",
    "        # align and warp each face\n",
    "        face_helper.align_warp_face()\n",
    "\n",
    "    # face restoration for each cropped face\n",
    "    for idx, cropped_face in enumerate(face_helper.cropped_faces):\n",
    "        # prepare data\n",
    "        cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n",
    "        normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n",
    "        cropped_face_t = cropped_face_t.unsqueeze(0).to(device)\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                output = net(cropped_face_t, w=w, adain=True)[0]\n",
    "                restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1))\n",
    "            del output\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as error:\n",
    "            print(f'\\tFailed inference for CodeFormer: {error}')\n",
    "            restored_face = tensor2img(cropped_face_t, rgb2bgr=True, min_max=(-1, 1))\n",
    "\n",
    "        restored_face = restored_face.astype('uint8')\n",
    "        face_helper.add_restored_face(restored_face)\n",
    "\n",
    "    # paste_back\n",
    "    if not args.has_aligned:\n",
    "        # upsample the background\n",
    "        if bg_upsampler is not None:\n",
    "            # Now only support RealESRGAN for upsampling background\n",
    "            bg_img = bg_upsampler.enhance(img, outscale=args.upscale)[0]\n",
    "        else:\n",
    "            bg_img = None\n",
    "        face_helper.get_inverse_affine(None)\n",
    "        # paste each restored face to the input image\n",
    "        restored_img = face_helper.paste_faces_to_input_image(upsample_img=bg_img, draw_box=args.draw_box)\n",
    "\n",
    "    # save faces\n",
    "    # for idx, (cropped_face, restored_face) in enumerate(zip(face_helper.cropped_faces, face_helper.restored_faces)):\n",
    "    #     # save cropped face\n",
    "    #     if not args.has_aligned: \n",
    "    #         save_crop_path = os.path.join(result_root, 'cropped_faces', f'{basename}_{idx:02d}.png')\n",
    "    #         imwrite(cropped_face, save_crop_path)\n",
    "    #     # save restored face\n",
    "    #     if args.has_aligned:\n",
    "    #         save_face_name = f'{basename}.png'\n",
    "    #     else:\n",
    "    #         save_face_name = f'{basename}_{idx:02d}.png'\n",
    "    #     save_restore_path = os.path.join(result_root, 'restored_faces', save_face_name)\n",
    "    #     imwrite(restored_face, save_restore_path)\n",
    "\n",
    "    # save restored img\n",
    "    if not args.has_aligned and restored_img is not None:\n",
    "        save_restore_path = os.path.join(result_root, 'final_results', f'{basename}.png')\n",
    "        imwrite(restored_img, save_restore_path)\n",
    "\n",
    "print(f'\\nAll results are saved in {result_root}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652c173-e113-4370-8732-b24969bd4f89",
   "metadata": {},
   "source": [
    "### Reconstruct restored video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d3b85-b07e-4352-81e0-5d45a87784d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "img_array = []\n",
    "for filename in glob.glob(f'{result_root}/final_results/*.png'):\n",
    "    img = cv2.imread(filename)\n",
    "    height, width, layers = img.shape\n",
    "    size = (width, height)\n",
    "    img_array.append(img)\n",
    "\n",
    "Path('../videos').mkdir(parents=True, exist_ok=True)\n",
    "out = cv2.VideoWriter('../videos/restored.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 1, size)\n",
    " \n",
    "for i in range(len(img_array)):\n",
    "    out.write(img_array[i])\n",
    "    \n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58f280a-2c19-4ca7-afa8-f939e6472e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.arj.no/2021/05/11/ffmpeg-audio-video/\n",
    "!ffmpeg -i ../$video_name -i ../videos/restored.mp4 -c copy -map 1:v:0 -map 0:a:0 -shortest ../videos/restored_w_audio.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b63bff-ce3f-4308-946d-24b46da081fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"data:video/mp4;base64,\" + b64encode(open('../videos/restored_w_audio.mp4', 'rb').read()).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearnear-default:Python",
   "language": "python",
   "name": "conda-env-machinelearnear-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
